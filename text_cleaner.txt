import re
import spacy

def clean_text(text):

  text = text.lower() # Coverts all the characters in the text to lowercase
  text = re.sub(r'http\S+|www\S+|https\S+', '', text) # Removes URL
  text = re.sub(r'<.*?>', '', text) # Removes HTML tags (ex: <div>, <p>, etc)
  text = re.sub(r'[^a-z\s]', '', text) # Removes numbers, punctuations and special characters
  text = re.sub(r'\s+', ' ', text).strip() # Removes leading/trailing and multiple spaces
  text = re.sub(r'\S+@\S+\.\S+', '', text) # Removes email addresses
  text = re.sub(r'[^\w\s]', '', text) # Removes punctuations (non word characters)
  text = re.sub(r'(.)\1{2,}', r'\1', text) # Removes repeated characters to normalise enlogated words
  text = re.sub(r'#', '', text) # Removes hashtag symbols
  text = re.sub(r'@\w+', '', text) # Removes user mentions
  text = re.sub(r'[^\x00-\x7F]+', '', text) # Removes non ASCII characters (ex: emojis and foreign characters)

  return text

nlp = spacy.load('en_core_web_sm')

def clean_text_spacy(text):

  doc = nlp(text)
  tokens = []

  for i in doc:
    if not i.is_stop and not i.is_punct:
      tokens.append(i.lemma_)

  return tokens