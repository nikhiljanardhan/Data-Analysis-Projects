# WORD CLOUD

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def word_cloud(text):
    try:
        wc = WordCloud(background_color='white', width=800, height=400).generate(" ".join(text))
        fig, ax = plt.subplots(figsize=(10, 5))  # Create a figure and axis
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        return fig
    except Exception as e:
        print(f'Error: {e}')

# N-GRAM ANALYSIS

from nltk.util import ngrams
from collections import Counter
import plotly.graph_objects as go
import streamlit as st

# Creating chart for n-gram analysis

def ngram_chart(tokens, n, top_n):
  try:
    ngram = list(ngrams(tokens, n))
    ngram_count = Counter(ngram).most_common(top_n)

    if not ngram_count:
      raise ValueError(f'No n-grams found')

    labels = []
    counts = []

    for i, j in ngram_count:
      labels.append(" ".join(i))
      counts.append(j)

    fig = go.Figure(data=[go.Bar(x=labels, y=counts, text=counts, textposition='outside')])

    fig.update_layout(
        height=500,
        title = f'Top {top_n} {n}-grams',
        xaxis_title = f'{n}-grams',
        yaxis_title = 'Count',
    )

    st.plotly_chart(fig)

  except Exception as e:
    print(f'Error: {e}')

# EMOTION DETECTION

# Function to create chunks

import spacy

def create_chunks(text, max_length = 500):

  nlp = spacy.load('en_core_web_sm')
  doc = nlp(text)
  chunks = []
  current_chunk = ''

  for i in doc.sents:
    sentence = i.text.strip()

    if len(current_chunk) + len(sentence) <= 500:
      current_chunk += ' '+sentence
    else:
      chunks.append(current_chunk.strip())
      current_chunk = sentence

  if current_chunk:
    chunks.append(current_chunk.strip())

  return chunks

# Creating function for emotion detection

import pandas as pd
import plotly.express as px
from transformers import pipeline

model_name = "j-hartmann/emotion-english-distilroberta-base"
emotion_classifier = pipeline("text-classification", model=model_name, tokenizer=model_name, top_k = None)

from collections import defaultdict

def detect_chunk_emotion(text):

  chunks = create_chunks(text)
  emotions_total = {}
  emotions_count = {}
  emotions_count = defaultdict(int)

  for i in chunks:
    result = emotion_classifier(i)[0]

    for j in result:
      label = j['label']
      score = j['score']
      emotions_total[label] = emotions_total.get(label, 0) + score
      emotions_count[label] += 1

    emotion_count = dict(emotions_count)


  emotions_average = {i: emotions_total[i]/emotions_count[i] for i in emotions_total}
  sorted_emotions = sorted(emotions_average.items(), key=lambda x: x[1], reverse = True)
  top_emotions = sorted_emotions[:5]
  df = pd.DataFrame(top_emotions, columns=['Emotion', 'Confidence'])

  return df

# SENTIMENT ANALYSIS

model_name= "cardiffnlp/twitter-roberta-base-sentiment"
sentiment_classifier= pipeline("sentiment-analysis", model= model_name,  tokenizer= model_name , return_all_scores=True)

# Creating Function for Sentiment Analysis

def sentiment_analysis(text):
  try:

    sentiment_labels = {'LABEL_0': 'Negative', 'LABEL_1': 'Neutral', 'LABEL_2': 'Positive'}

    chunks = create_chunks(text)
    total_score = {'Negative': 0, 'Neutral':0, 'Positive':0}
    chunk_count = len(chunks)

    for i in chunks:
      result = sentiment_classifier(i)[0]

      for res in result:
        label = sentiment_labels[res['label']]
        total_score[label] += res['score']

    average_score = {}

    for j in total_score:
      average_score[j] = total_score[j]/chunk_count

    overall_sentiment = max(average_score, key=average_score.get)

    return {
        'Overall Sentiment':overall_sentiment,
        'Average Score':average_score
    }

  except Exception as e:
    print(f'Error: {e}')

# TONE OF SPEECH DETECTION

LABELS = ["Factual", "Opinion", "Question", "Command", "Angry", "Sadness", "Surprise", "Persuasive", "Humorous", "Sarcastic",
          "Suggestion", "Prediction", "Warning", "Narrative", "Argument", "Confident", "Doubtful", "Inappropriate", "Information"]

# initialize once (do this outside the function for speed)
nlp = spacy.load("en_core_web_sm")


# zero-shot classifier
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")


def classify_text(text,
                  labels= LABELS,
                  chunk_size=5,
                  overlap=2):

    if not isinstance(text, str):
        raise ValueError("text must be a string")

    doc = nlp(text)

    # Split into sentences
    sentences = [sent.text.strip() for sent in doc.sents]
    if not sentences:
        return {"Text": text, "Predicted_Category": None, "Score": 0.0, "All_Categories": []}

    # If short enough, classify whole text at once
    if len(sentences) <= chunk_size:
        result = classifier(text, candidate_labels=labels)
        all_categories = list(zip(result["labels"], result["scores"]))
        return {
            "Text": text,
            "Predicted_Category": result["labels"][0],
            "Score": float(result["scores"][0]),
            "All_Categories": all_categories
        }

    # Build overlapping chunks (each chunk is a list of sentences)
    chunks = []
    start = 0
    while start < len(sentences):
        end = start + chunk_size
        chunk = sentences[start:end]
        chunks.append(chunk)
        if end >= len(sentences):
            break
        start += (chunk_size - overlap)

    # Aggregate scores across chunks
    total_scores = {label: 0.0 for label in labels}
    for chunk in chunks:
        chunk_text = " ".join(chunk)  # <-- important fix: pass string, not list
        result = classifier(chunk_text, candidate_labels=labels)

        # result["labels"] are returned in descending score order; map them to their scores
        for label, score in zip(result["labels"], result["scores"]):
            total_scores[label] += float(score)

    # Average and sort
    num_chunks = len(chunks)
    avg_scores = {label: (total_scores[label] / num_chunks) for label in labels}
    sorted_categories = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)
    top_label, top_score = sorted_categories[0]

    return {
        "Predicted_Category": top_label,
        "Score": float(top_score),
        "All_Categories": sorted_categories
    }

# SUMMARY GENERATION

#Creating Function for Text Summarization

def summarize(text):
  summarizer = pipeline('summarization', model='facebook/bart-large-cnn')
  chunks = create_chunks(text, max_length=500)

  # Summarize each chunk
  chunk_summaries = []
  for i in chunks:
    input_length = len(i.split())
    max_summary_length = int(input_length * 0.8)
    min_summary_length = int(input_length * 0.3)
    summary = summarizer(i, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)[0]["summary_text"]
    chunk_summaries.append(summary)

  # Combining summaries of all the chunks
  combined_summary = " ".join(chunk_summaries)

  # Final Summary of combined summary

  input_length = len(combined_summary.split())
  max_summary_length = int(input_length * 0.8)
  min_summary_length = int(input_length * 0.3)
  final_summary = summarizer(combined_summary, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)[0]["summary_text"]

  return final_summary